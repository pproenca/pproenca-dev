---
title: "I Won a Prize in a Coding Competition Without Writing Code"
date: "2025-01-09"
description: "I found a $10k WebCodecs competition 48 hours before the deadline. I'd never written C++ bindings. So I didn't — I orchestrated AI agents to write every line."
categories: ["AI", "Software Development"]
---

Two days. That's how much time I had when I stumbled across [vjeux's WebCodecs Node.js challenge](https://github.com/vjeux/webcodecs-nodejs-10k-challenge). Ten thousand dollars to implement the WebCodecs API server-side. The kind of challenge I would've scrolled past a year ago, thinking "that's not for me."

<Tweet id="1994872345610391748" />

I'd never written a Node.js addon in my life. C++ bindings to FFmpeg. Not exactly my Tuesday afternoon. But I had Claude Code. And I had [my own plugin](https://github.com/pproenca/dot-claude/) for orchestrating it.

So I tried something different. Instead of writing implementation code, I wrote specs. I pulled the official WebCodecs IDL and turned it into detailed implementation plans. Claude Code wrote the C++ bindings, the N-API wrappers, the TypeScript interfaces.

My job was to review, steer, and fix the process when it broke.

And it broke a lot.

## The Assumption Trap

Here's the thing about coding with AI agents: they're confident. Always. Even when they're wrong.

Claude would hit a bug and declare "found it!" with absolute certainty. No reproduction. No evidence. Just pattern-matching from training data. It had seen similar stack traces before, so it assumed the same fix would work.

Tests were passing. The code looked clean. But those tests were validating the wrong implementation, not the correct behaviour. I've seen this in management too — metrics lie if you're measuring the wrong things.

The AI was guessing. And I was letting it.

## Prove It First

Second iteration, I changed the rules. Before Claude could claim it fixed anything, it had to prove the bug existed first. Reproduce it. Show me the failing test. Only then fix it.

I built skills for debugging tools like `leaks` on macOS. Updated my CLAUDE.md with failure patterns: what the error looks like, what tool catches it. A whole debugging workflow, encoded.

<Tweet id="2006425822174040345" />

I made a joke early on. "I hope I run into a memory segmentation fault." Hours later, that's exactly what I was debugging. C++ doesn't forgive. But now Claude had the tools to actually fix it, not just guess.

That's when things started moving fast.

## The Win

<Tweet id="2006254902629044564" />

I [submitted](https://github.com/vjeux/webcodecs-nodejs-10k-challenge/issues/8) with 428 out of 442 tests passing — 96.8%. A working Muxer and Demuxer that went beyond the W3C spec. Seven of us got solutions working, something vjeux didn't think would happen. We split the $10k, $1,500 each.

Two days. Zero Node addon experience. One working WebCodecs implementation.

But here's what actually changed: the skill isn't writing code anymore. It's orchestrating agents, teaching them to be rigorous, and knowing when they're guessing. The gap between "I could never do that" and "let me try" has collapsed — but you have to know how to work on the other side of it.

I'm making this open source. I want to render videos server-side, and I want others to build on it.

If you want to learn AI-assisted development, don't build another website. Find something that scares you. Something you'd scroll past thinking "that's not for me."

---

[Check out the repo](https://github.com/pproenca/node-webcodecs/) | [My Claude Code plugin](https://github.com/pproenca/dot-claude/)
